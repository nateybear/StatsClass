\documentclass{article}
\usepackage{\string ~/LATEXStyle}
\title{Stat 574B: HW 4 Problem 1}
\author{Nate Hattersley}
\date{November 15, 2017}
\Sexpr{opts_chunk$set(tidy=T, out.width=".6\\linewidth", fig.align="center",tidy.opts=list(width.cutoff=60))}
<<echo=F>>=
library(xtable)
library(magrittr)
library(ggplot2)
library(car)
library(coda)
library(gridExtra)
@
\begin{document}
\maketitle

\subsection*{part a.}
Assume Y and $\theta$ are distributed as given. Then the integrand $p(Y|\theta)p(\theta)$ looks like:
\begin{equation*}
p(Y|\theta)p(\theta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{n+\alpha-1}e^{-\theta(\beta+\sum\limits_iY_i)}
\end{equation*}
Integrating out theta, we recognize the terms dependent on theta as the kernel of a Gamma($n+\alpha$, $\beta+\sum\limits_iY_i$). This means that

\begin{align*}
p(Y) &= \frac{\beta^\alpha}{\Gamma(\alpha)}\int\limits_0^\infty\theta^{n+\alpha-1}e^{-\theta(\beta+\sum\limits_iY_i)}d\theta\\
&=\frac{\beta^\alpha}{\Gamma(\alpha)}\frac{\Gamma(n+\alpha)}{(\beta+\sum\limits_iY_i)^{n+\alpha}}
\end{align*}

I'm not sure how to interpret the results of this, since it seems that $\alpha, \beta, n$ are fixed. But, it's simple enough to see that you can factor the PDF according to Neyman-Fisher to show that $\sum\limits_iY_i$ is a sufficient statistic. The average is a one-to-one transformation of the sum, hence $\bar{Y}$ is also sufficient.

\subsection*{part b.}

This predictive PDF is obviously a decreasing function of $\sum\limits_iY_i$ and by extension $\bar{Y}$. Hence if $x>y$, $p(x)<p(y)$; and if $p(x)<p(y)$, $x>y.\quad\blacksquare$

\subsection*{part c.}

<<pc1, echo=F>>=
## PART C
set.seed(42)
yobs <- c(65, 156, 100, 134, 16, 108,
          121, 4, 39, 143, 56, 26, 22,
          1, 1, 5, 65)
n <- 17; a <- 1; b <- 52

size <- 1000

## Generate PPD Sample
theta_new <- rgamma(17*size, n+a, b+sum(yobs))
ynew <- rexp(17*size, theta_new)
@

The sampling scheme is relatively straightforward. First, I will sample from the posterior distribution of $\theta|Y_{obs}$, and use those samples as parameters to sample from the exponential distribution. These are the posterior predictive samples. I took \Sexpr{17*size} samples. I create a table with summary statistics (mean, standard deviation, 95\% credible interval) of the PPD sample as well as a histogram.

<<pc2, echo=F, results='asis'>>=
## Summaries of PPD
ynew %>% (function(x){data.frame(Mean=mean(x),SD=sd(x),Lower=quantile(x,.025),Upper=quantile(x, .975))}) %>% xtable %>% print(include.rownames=F)

ggplot(data.frame(Weeks = ynew)) + geom_histogram(aes(x = Weeks, y = ..density..), binwidth = 20) +
  ggtitle("Posterior Predicted Survival Times for Leukemia Patients") +
  geom_vline(aes(xintercept = mean(yobs)), col = "red") +
  geom_text(
  aes(
  x = mean(yobs) + 110,
  y = .009,
  label = "Observed Mean"
  ),
  alpha = 0.75,
  family = "mono",
  fontface = "plain",
  size = 5
  )

## Computing P value

p.val <- 0

chunk_i <- function(i) {
  ynew[(17*i+1):(17*(i+1))]
}

for(i in 0:(size-1)) {
  subst <- chunk_i(i)
  p.val %<>% c(mean(subst) > mean(yobs)) %>% weighted.mean(c(i,1))
}

@

Now, to model the p-value, note that we can break the output vector into \Sexpr{size} samples of 17 numbers. Hence, I will iterate over the PPD sample in chunks of 17 and compare the chunk's sample mean to our data mean. The proportion of chunks with sample averages greater than observed is the p-value, which I found to be \Sexpr{p.val}. Given such a large p value, we conclude that we fail to reject the null assumption of the data being exponentially distributed.

\subsection*{part d.}

For this part, I will just randomly pick 9 chunks of 17 like I mentioned above, and make QQ plots for each one of them. First, I include the QQ plot of the observed data for reference:

<<pd1, echo=F, out.height=".3\\paperheight">>=
## PART D
qqPlot(yobs, d="exp")
par(mfrow=c(3,3))
for(i in 1:9){
  qqPlot(rexp(17),dist="exp")
}
@

Qualitatively, it looks like there is room for huge outliers from generating a small sample of the exponential distribution. My conclusion is that our finding from part c is justified, and that more anamolous results than observed are possible from a random sample of 17 exponential random variables.

\pagebreak
\subsection*{part e.}

My starting value for $\theta$ was $1/\bar{Y}$, where $\bar{Y}$ is the data mean. This was my naive best guess for $E(\theta)$ and should lie in a region with good probability mass. I chose a selection of $\delta$ values of different orders of magnitude ($10^{-i}$ for $i\in 1:4$) in order to visualize the effects on the acceptance ratio and the trace plot. If $\delta$ is too large, then the acceptance ratio becomes exceedingly small and you won't get a good approximation of the posterior. I figured that the best $\delta$ value would be somewhere close to the standard deviation of the distribution, perhaps some small multiple of it. I know the posterior and so I know that its standard deviation is .0038, hence I posit that .01 and .001 will provide the best mixing.
\noindent\rule{\textwidth}{1pt}\\\\

<<pe1, echo=F, results='asis', out.height=".15\\paperheight", out.width=".3\\linewidth">>=
## PART E
size <- 1000

test.delta <- function(d) {
  theta <- 1/mean(yobs)
  THETA <- c()
  ACCEPTRATIO <- 0
  for(i in 1:size) {
    
    theta_prop <- runif(1, theta-d, theta+d)
    dtheta <- dgamma(theta, n+a, b+sum(yobs))
    dtheta_prop <- dgamma(theta_prop, n+a, b+sum(yobs))
    
    if(!dtheta || is.na(dtheta)) 
      accept <- 1
    else if(is.na(dtheta_prop))
      accept <- 0
    else
      accept <- dtheta_prop/dtheta
    
  
    if(accept > runif(1)) {
      ACCEPTRATIO %<>% c(1) %>% weighted.mean(c(i-1,1))
      theta <- theta_prop
    } else {
      ACCEPTRATIO %<>% c(0) %>% weighted.mean(c(i-1,1))
    }
    THETA[i] <- theta
  }
  list(MCMC=as.mcmc(THETA), RATIO=ACCEPTRATIO)
}

test.deltas <- function(...) {
  for(i in c(...)) {
    out <- test.delta(i)
    cat(sprintf("Delta: %.3f\\\\",i))
    cat(sprintf("Acceptance Ratio: %.2f\\%%\\\\", 100*out$RATIO))
    cat(sprintf("Effective Size: %.2f",effectiveSize(out$MCMC)))
    plot(out$MCMC)
    acf(out$MCMC)
    cat("\\noindent\\rule{\\textwidth}{1pt}\\\\\\\\")
  }
}

test.deltas(1,.1,.01, .001)

@

Interestingly, $\delta=.001$ provided poor mixing because the delta value was so small that almost every point was accepted. The best mixing came when $\delta=.01$. To illustrate what that means for patient outcomes, I took the mean, median, and a 95\% credible interval on the MH posterior sample. The mean of the resultant exponential distribution given $\theta$ is $1/\theta$. Hence, I inverted the summaries of $\theta$ to give an idea of the possible spread of population means.

<<pe2, echo=F, results='asis'>>=
## one-liner to make a variable and print it
(best <- test.delta(.01) %>% with(
  data.frame(
    Mean=1/mean(MCMC),
    Median=1/median(MCMC),
    Lower=1/quantile(MCMC, .975),
    Upper = 1/quantile(MCMC, .025)
  )
)) %>% xtable %>% print(include.rownames=F)
@

\pagebreak
\subsection*{Code}
<<eval=F, tidy=T, split=TRUE>>=
<<pc1>>
<<pc2>>
<<pd1>>
<<pe1>>
<<pe2>>
@

\end{document}